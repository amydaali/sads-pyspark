{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/sads-logo.jpeg\" align=\"left\" width=\"14%\">\n",
    "\n",
    "<img src=\"images/spark-logo-hd.png\" align=\"right\" width=\"21%\">\n",
    "<img src=\"images/python-logo-notext.png\" align=\"right\" width=\"11%\">\n",
    "\n",
    "<h1 align='center'>Intro to PySpark Workshop</h1>\n",
    "<h2 align='center'>Meghann Agarwal</h2>\n",
    "<h3 align='center'>September 14, 2017</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bottlenecks and Blockers\n",
    "#### CPU Bound\n",
    "    - Optimize algorithms\n",
    "    - Optimize compilation\n",
    "    - Parallelize code\n",
    "#### Storage Bound\n",
    "    - Store data elsewhere and transfer to compute machine when needed\n",
    "    - Spread data across multiple machines and run computations on all machines\n",
    "#### Network I/O Bound\n",
    "    - Decrease network transfer time\n",
    "        - Compress data for faster transfer\n",
    "        - Move computation to the data\n",
    "    - Consider multithreading\n",
    "        - Unblock code by allowing data to transfer in a separate thread\n",
    "#### Disk I/O  Bound\n",
    "    - Cache data in RAM for faster access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Outline\n",
    "- **Definitions:**\n",
    "    - MapReduce\n",
    "    - Hadoop\n",
    "- **PySpark:**\n",
    "    - Exercises:\n",
    "        - Word Count\n",
    "        - Logistic Regression\n",
    "        - Clickstream\n",
    "    - References and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is MapReduce?\n",
    "https://research.google.com/archive/mapreduce.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Why MapReduce?\n",
    "\n",
    ">\"The primary benefit of MapReduce is that it allows us to distribute computations by moving the processing to the data.\"  \n",
    "-- J. Grus, *Data Science from Scratch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Hadoop?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Spark Deployment Modes\n",
    "- Spark Standalone Cluster Mode\n",
    "- Spark on Hadoop YARN Cluster\n",
    "- Spark on Apache Mesos Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Installation Options\n",
    "Here are a few options for \"installing\" Spark:\n",
    "\n",
    "### Databricks Community Edition\n",
    "1. Use Google Chrome browser (Firefox should also work, but not Internet Explorer, Safari, etc.)\n",
    "2. Sign up for a free Community Edition account here: https://databricks.com/try-databricks\n",
    "\n",
    "### Local on Mac OSX\n",
    "1. Download http://apache.spinellicreations.com/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\n",
    "2. Extract the Spark package and create `SPARK_HOME`\n",
    "```\n",
    "tar -xvzf spark-2.2.0-bin-hadoop2.7.tgz\n",
    "sudo mv spark-2.2.0-bin-hadoop2.7 /opt/spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "3. Run the included Pi Estimator example by executing the following command:\n",
    "```\n",
    "$SPARK_HOME/bin/run-example SparkPi 10\n",
    "```\n",
    "Expect to see something like:\n",
    "```\n",
    "Pi is roughly 3.140576\n",
    "```\n",
    "\n",
    "### Amazon Web Services (AWS)\n",
    "1. Deploy Spark on Elastic Cloud Compute (EC2) cluster\n",
    "2. Deploy Spark on Elastic MapReduce (EMR) cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Spark?\n",
    "Spark is a fast and expressive cluster computing system for doing Big Data computation. It's good for:\n",
    "- iterative tasks\n",
    "- doing big batch processing\n",
    "- interactive data exploration\n",
    "\n",
    "It's compatible with Hadoop-supported file systems and data formats (HDFS, S3, SequenceFile, ...), so if you've been using Hadoop you can use it with your existing data and deploy it on your existing clusters.\n",
    "\n",
    "It achieves fault tolerance through *lineage*: if you lose a partition (chunk) of data you can reconstruct it through a set of *transformations* that act on data stored in memory. This is in contrast to distributed shared memory systems where you have to write to disk and roll back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use Spark?\n",
    "https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/speed.png\" align=\"left\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/ease-of-use.png\" align=\"left\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/generality.png\" align=\"left\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/runs-everywhere.png\" align=\"left\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    ">\"Although current frameworks provide numerous abstractions for accessing a cluster’s computational resources, they <font color='red'> lack abstractions for leveraging distributed memory</font>. This makes them inefficient for an important class of emerging applications: those that reuse intermediate results across multiple computations. <font color='blue'>Data reuse is common in many iterative machine learning and graph algorithms</font>, including PageRank, K-means clustering, and logistic regression. <font color='blue'>Another compelling use case is interactive data mining</font>, where a user runs multiple ad-hoc queries on the same subset of the data. Unfortunately, in most current frameworks, the only way to reuse data between computations (e.g., between two MapReduce jobs) is to <font color='red'>write it to an external stable storage system</font>, e.g., a distributed file system. This incurs substantial overheads due to data replication, disk I/O, and serialization, which can dominate application execution times.\"  \n",
    "-- Zaharia et al., \"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,\" *In NSDI '12*, April 2012\n",
    "\n",
    "https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark vs MapReduce vs MPI vs ...\n",
    "- [MapReduce](https://en.wikipedia.org/wiki/MapReduce) --> [Hadoop](http://hadoop.apache.org/): heavily used in business computing\n",
    "- [Message Passing Interface (MPI)](https://en.wikipedia.org/wiki/Message_Passing_Interface) --> [MVAPICH](http://mvapich.cse.ohio-state.edu/): heavily used in scientific computing\n",
    "- [Spark](http://spark.apache.org/): complement to Hadoop, faster for iterative applications, rich set of APIs in Scala, Python, and Java, and an interactive shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Architecture\n",
    "- Spark Driver and Workers\n",
    "- SparkContext (replaced by SparkSession in version 2.X)\n",
    "- Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).\n",
    "- SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN)\n",
    "\n",
    "<img src=\"images/cluster-overview.png\" align=\"center\" width=\"75%\">\n",
    "\n",
    "<h4 align='right'>https://spark.apache.org/docs/1.1.0/cluster-overview.html</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Programming Concepts (version 1.X)\n",
    "- ****SparkContext****: entry point to Spark functions\n",
    "- ****Resilient Distributed Datasets (RDDs)****:\n",
    "    - Immutable, distributed collections of objects\n",
    "    - Can be cached in memory for fast reuse\n",
    "- ****Operations on RDDs****:\n",
    "    - *Transformations*: define a new RDD (map, join, ...)\n",
    "    - *Actions*: return or output a result (count, save, ...)\n",
    "- ****Two ways to create RDDs****:\n",
    "    1. By parallelizing an existing collection in your driver program:  \n",
    "        `data = [1, 2, 3, 4, 5]  \n",
    "        distData = sc.parallelize(data)`  \n",
    "    2. Or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat:  \n",
    "        `distFile = sc.textFile(\"data.txt\")`       \n",
    "<h4 align='right'>http://spark.apache.org/docs/latest/programming-guide.html</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Data Interfaces\n",
    "\n",
    "- [RDD API](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n",
    "- [DataFrame API](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes)\n",
    "- [Machine Learning API](https://spark.apache.org/docs/latest/mllib-guide.html)\n",
    "\n",
    "There are several key interfaces that you should understand when you go to use Spark.\n",
    "\n",
    "-   ****The Dataset****\n",
    "    -   The Dataset is Apache Spark's newest distributed collection and can be considered a combination of DataFrames and RDDs. It provides the typed interface that is available in RDDs while providing a lot of conveniences of DataFrames. It will be the core abstraction going forward.\n",
    "-   ****The DataFrame****\n",
    "    -   The DataFrame is collection of distributed `Row` types. These provide a flexible interface and are similar in concept to the DataFrames you may be familiar with in python (pandas) as well as in the R language.\n",
    "-   ****The RDD (Resilient Distributed Dataset)****\n",
    "    -   Apache Spark's first abstraction was the RDD or Resilient Distributed Dataset. Essentially it is an interface to a sequence of data objects that consist of one or more types that are located across a variety of machines in a cluster. RDD's can be created in a variety of ways and are the \"lowest level\" API available to the user. While this is the original data structure made available, new users should focus on Datasets as those will be supersets of the current RDD functionality.\n",
    "\n",
    "*(slide taken from \"Introduction to Apache Spark on Databricks\" notebook)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is PySpark?\n",
    "- The Python API for Spark\n",
    "- Run interactive jobs in the shell\n",
    "- Supports numpy, pandas and other Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Why use PySpark?\n",
    "- If you already know Python\n",
    "- Can use Spark in tandem with your favorite Python libraries\n",
    "- If you don't need Python libraries, maybe just write code in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PySpark's core classes (version 1.X):\n",
    "- ****pyspark.SparkContext****  \n",
    "Main entry point for Spark functionality.\n",
    "- ****pyspark.RDD****  \n",
    "A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
    "- ****pyspark.streaming.StreamingContext****  \n",
    "Main entry point for Spark Streaming functionality.\n",
    "- ****pyspark.streaming.DStream****  \n",
    "A Discretized Stream (DStream), the basic abstraction in Spark Streaming.\n",
    "- ****pyspark.sql.SQLContext****  \n",
    "Main entry point for DataFrame and SQL functionality.\n",
    "- ****pyspark.sql.DataFrame****  \n",
    "A distributed collection of data grouped into named columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformations\n",
    "- Transform one RDD to another, **new** RDD (immutable)\n",
    "\n",
    "| Transformation | Description | Type |\n",
    "| :------:  | :-----------: | :-----: |\n",
    "| `map(func)`     | Apply a function over each element | Narrow |\n",
    "| `flatMap(func)` | Map then flatten output | Narrow |\n",
    "| `filter(func)`  | Keep only elements where function is `True` | Narrow |\n",
    "| `sample(withReplacement, fraction, seed)` | Return a sampled subset of this RDD | Narrow |\n",
    "| `groupByKey(k, v)` | Group the values for each key in the RDD into a single sequence | Wide |\n",
    "| `reduceByKey(func)` | Merge the values for each key using an associative reduce function | Wide |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/narrow_wide_transformations.png\" align=\"center\" width=\"90%\">\n",
    "\n",
    "<h4 align='right'>https://dzone.com/articles/big-data-processing-spark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Actions\n",
    "- Return or output a result\n",
    "\n",
    "| Action | Description | Try it Out\\*|\n",
    "| :------:  | :-----------:| :---: |\n",
    "| `collect()`     | Return a list that contains all of the elements in this RDD | `sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()` |\n",
    "| `count()`  | Return the number of elements | `sc.parallelize([2, 3, 4]).count()` |\n",
    "| `saveAsTextFile(path)` | Save as a text file, using string representations of elements | `sc.parallelize(['foo', '-', 'bar', '!']).saveAsTextFile(\"/FileStore/foo-bar.txt\")])`|\n",
    "| `first()`    | Return the first element | `sc.parallelize([2, 3, 4]).first()` |\n",
    "| `take(num)`    | Take the first num elements | `sc.parallelize([2, 3, 4, 5, 6]).take(2)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \\* Try it Out:\n",
    "1. Go to your databricks Workspace and create a new directory within your Users directory called \"2017-09-14-sads-pyspark\" \n",
    "2. Create a notebook called \"0-Introduction\"  within this directory\n",
    "3. Type or copy/paste lines of code into separate cells and run them (you will be prompted to launch a cluster) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When using Databricks the `SparkContext` is created for you automatically as `sc`.\n",
    "\n",
    "In the Databricks Community Edition there are no Worker Nodes - the Driver Program (Master) executes the entire code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try a couple more examples with transformations and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())\n",
    "sorted(rdd.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown only a subset of possible *transformations* and *actions*. Check out others for your application in the docs: http://spark.apache.org/docs/latest/api/python/pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Log Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "val lines = spark.textFile(\"hdfs://...\")\n",
    "val errors = lines.filter(_.startsWith(\"ERROR\"))\n",
    "val messages = errors.map(_.split('\\t')(2))\n",
    "\n",
    "messages.filter(_.contains(\"foo\")).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The computation is expressed declaratively and nothing actually takes place until calling `count` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1: Word Count\n",
    "Create a few transformations to build a dataset of (String, Int) pairs called counts and then save it to a file.\n",
    "\n",
    "1. Create a notebook in \"2016-06-20-pyladies-pyspark\" called \"1-WordCount\"\n",
    "2. Try to implement the following Word Count example:\n",
    "\n",
    "http://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs://...\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2: Logistic Regression\n",
    "\n",
    "1. Create a notebook in \"2016-06-20-pyladies-pyspark\" called \"2-LogisticRegression\"\n",
    "2. Try to implement one of the following Logistic Regression examples:\n",
    "    - http://spark.apache.org/examples.html (Prediction with Logistic Regression)\n",
    "    - https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/logistic_regression.py\n",
    "    - https://github.com/apache/spark/blob/master/examples/src/main/python/logistic_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Every record of this DataFrame contains the label and\n",
    "# features represented by a vector.\n",
    "df = sqlContext.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Set parameters for the algorithm.\n",
    "# Here, we limit the number of iterations to 10.\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Fit the model to the data.\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 3: Clickstream\n",
    "\n",
    "***Switch over to Databricks***\n",
    "- Import notebook \"3-Clickstream\" in to \"2017-09-14-sads-pyspark\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***Objective: Get some practice using Spark DataFrames***\n",
    "\n",
    "***Data Source: February 2015 English Wikipedia Clickstream dataset***\n",
    "- https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream\n",
    "- http://datahub.io/dataset/wikipedia-clickstream/resource/be85cc68-d1e6-4134-804a-fd36b94dbb82. \n",
    "\n",
    ">\"The data contains counts of (referer, resource) pairs extracted from the request logs of English Wikipedia. When a client requests a resource by following a link or performing a search, the URI of the webpage that linked to the resource is included with the request in an HTTP header called the \"referer\". This data captures 22 million (referer, resource) pairs from a total of 3.2 billion requests collected during the month of February 2015.\"\n",
    "\n",
    "The data is approximately 1.2GB and it is hosted in the following Databricks file: `/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed`\n",
    "\n",
    "*Notebook translated from the Databricks \"Quick Start DataFrames\" tutorial in Scala, which was based on a lab developed by [Sameer Farooqui](https://www.linkedin.com/in/blueplastic).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources and References\n",
    "- ****MOOCs****:\n",
    "    - \"Introduction to Apache Spark\": https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x\n",
    "    - \"Hadoop Platform and Application Framework\" (week 5 covers Spark): https://www.coursera.org/learn/hadoop/home/week/5\n",
    "- ****Spark/PySpark Docs****:\n",
    "    - (v2.0.0) http://spark.apache.org/docs/2.0.0-preview/\n",
    "    - http://spark.apache.org/research.html\n",
    "    - http://spark.apache.org/examples.html\n",
    "- ****Other****:\n",
    "    - Spark 2.0 Webinar (2016): http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin\n",
    "    - PySpark Talk (J. Rosen, 2013): https://www.youtube.com/watch?v=xc7Lc8RA8wE\n",
    "    - \"Apache Spark in 24 Hours\": https://www.amazon.com/Apache-Spark-Hours-Teach-Yourself/dp/0672338513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thanks for Coming!\n",
    "\n",
    "<agarwal.meghann@gmail.com>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
